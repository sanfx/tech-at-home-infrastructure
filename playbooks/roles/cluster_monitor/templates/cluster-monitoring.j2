version: '3.7'

networks:
  {{traefik_network_name}}:
    external: true
    name: {{traefik_network_name}}

volumes:
    prometheus: {}
    alertmanager: {}

configs:
  prometheus_config:
    file: /etc/prometheus/prometheus.yml
  node_rules:
    file: /home/{{ansible_user}}/stacks/prometheus/rules/swarm_node.rules.yml
  task_rules:
    file: /home/{{ansible_user}}/stacks/prometheus/rules/swarm_task.rules.yml

services:
  dockerd-exporter:
    image:  stefanprodan/dockerd-exporter:latest
    networks:
      - {{traefik_network_name}}
    environment:
      - DOCKER_GWBRIDGE_IP=172.18.0.1
    deploy:
      labels:
        - "diun.enable=true"
      replicas: 0
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  cadvisor:
    image: google/cadvisor
    networks:
      - {{traefik_network_name}}
    command: -logtostderr -docker_only
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    deploy:
      labels:
        - "diun.enable=true"
      replicas: 1
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  grafana:
    image: bankainojutsu/grafana-multiarch:latest
    user: "root:root"
    networks:
      - {{traefik_network_name}}
    environment:
      - "GF_SECURITY_ADMIN_USER={{GF_SECURITY_ADMIN_USER}}"
      - "GF_SECURITY_ADMIN_PASSWORD={{GF_SECURITY_ADMIN_PASSWORD}}"
      - "GF_USERS_ALLOW_SIGN_UP={{GF_USERS_ALLOW_SIGN_UP}}"
      - "GF_PATHS_LOGS: /var/log/grafana"
      - "GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel"
      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-}
      #- GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}
      #- GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@test.com}
      #- GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana}
      #- GF_SMTP_HOST=${GF_SMTP_HOST:-smtp:25}
      #- GF_SMTP_USER=${GF_SMTP_USER}
      #- GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD}
    volumes:
      - /vagrant/data/grafana/data:/var/lib/grafana
    ports:
      - "{{grafana_listen_port}}:{{grafana_listen_port}}"
    deploy:
      labels:
        - "diun.enable=true"
        - "traefik.enable=true"
        - "traefik.http.routers.grafana.rule=Host(`{{grafana_app_name}}.{{app_domain_name}}`)"
        - "traefik.http.middlewares.do-compress.compress=true"
        - "traefik.http.routers.grafana.middlewares=do-compress"
        - "traefik.http.services.grafana.loadbalancer.server.port={{grafana_listen_port}}"
        - "traefik.http.routers.grafana.entrypoints={{traefik_network_name}}"
        - "traefik.docker.network={{traefik_network_name}}"
      mode: replicated
      replicas: 1
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  alertmanager:
    image: stefanprodan/swarmprom-alertmanager:v0.14.0
    networks:
      - {{traefik_network_name}}
    environment:
      - "SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}"
      - "SLACK_CHANNEL=${SLACK_CHANNEL:-general}"
      - "SLACK_USER=${SLACK_USER:-alertmanager}"
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - alertmanager:/alertmanager
    deploy:
      labels:
        - "diun.enable=true"
      mode: replicated
      replicas: 0
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  unsee:
    image: cloudflare/unsee:v0.8.0
    networks:
      - {{traefik_network_name}}
    environment:
      - "ALERTMANAGER_URIS=default:http://alertmanager:9093"
    deploy:
      labels:
        - "diun.enable=true"
        - "traefik.enable=true"
        - "traefik.http.routers.unsee.rule=Host(`{{unsee_app_name}}.{{app_domain_name}}`)"
        - "traefik.http.services.unsee.loadbalancer.server.port={{unsee_listen_port}}"
        - "traefik.http.routers.unsee.entrypoints={{traefik_network_name}}"
        - "traefik.docker.network={{traefik_network_name}}"
      mode: replicated
      replicas: 0
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure

  node-exporter:
    image: stefanprodan/swarmprom-node-exporter:v0.16.0
    networks:
      - {{traefik_network_name}}
    environment:
      - NODE_ID="{{ansible_facts['nodename']}}"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.textfile.directory=/etc/node-exporter/'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.ipvs'
    deploy:
      labels:
        - "diun.enable=true"
      mode: global
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M
      restart_policy:
        condition: on-failure

  prometheus:
    image: prom/prometheus:v2.5.0
    networks:
      - {{traefik_network_name}}
    volumes:
      - "/home/{{ansible_user}}/stacks/prometheus/data:/prometheus:rw"
      - /etc/localtime:/etc/localtime:ro
      - /etc/prometheus:/etc/prometheus:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention=${PROMETHEUS_RETENTION:-24h}"
    logging:
      driver: "json-file"
    user: "root:root"
    deploy:
      labels:
        - "diun.enable=true"
        - "traefik.enable=true"
        - "traefik.http.routers.unsee.rule=Host(`{{prometheus_app_name}}.{{app_domain_name}}`)"
        - "traefik.http.services.unsee.loadbalancer.server.port={{prometheus_web_port}}"
        - "traefik.http.routers.unsee.entrypoints={{traefik_network_name}}"
        - "traefik.docker.network={{traefik_network_name}}"
      mode: replicated
      replicas: 1
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - "node.role==manager"

  diun:
    image: crazymax/diun:latest
    container_name: diun
    command: serve
    volumes:
      - "/vagrant/data/diun:/data"
      - "/home/{{ansible_user}}/stacks/cluster_monitoring/diun.yml:/diun.yml:ro"
      - "/var/run/docker.sock:/var/run/docker.sock"
    environment:
      - "TZ=Europe/London"
      - "LOG_LEVEL=info"
      - "LOG_JSON=false"
    networks:
      - {{traefik_network_name}}
    deploy:
      labels:
        - "diun.enable=true"
      mode: replicated
      replicas: 1
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - "node.role==manager"

  uptime-kuma:
    image: louislam/uptime-kuma:1
    hostname: "{{uptime_kuma_app_name}}"
    networks:
      - {{traefik_network_name}}
    volumes:
      - /vagrant/data/uptime_kuma:/app/data
    deploy:
      labels:
          - "diun.enable=true"
          - "traefik.enable=true"
          - "traefik.http.routers.uptime_kuma-http.rule=Host(`{{uptime_kuma_app_name}}.{{app_domain_name}}`)"
          - "traefik.http.routers.uptime_kuma-http.entrypoints={{traefik_network_name}}"
          - "traefik.http.routers.uptime_kuma-http.service=uptime_kuma-http-svc"
          - "traefik.http.services.uptime_kuma-http-svc.loadbalancer.server.port={{uptime_kuma_app_port}}"
          - "traefik.http.services.uptime_kuma-http-svc.loadbalancer.passhostheader=true"
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
           - "node.role==manager"
